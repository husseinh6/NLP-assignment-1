{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDMNWt0gxCU5",
        "outputId": "365e96c1-f6bd-41ba-f928-7b5184ccff14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#Importing all the necessary packages\n",
        "\n",
        "import pandas as pd # For data reading\n",
        "import chardet # For detecting the correct encoding of the data\n",
        "import re # For preprocessing\n",
        "from nltk.corpus import stopwords # For removing stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec # For Skip-gram and CBOW\n",
        "import torch\n",
        "import torch.nn as nn # for perceptron classifier\n",
        "import torch.optim as optim # to optimise the loss function\n",
        "\n",
        "\n",
        "# Downloading\n",
        "import nltk\n",
        "nltk.download('punkt') #tokenizer\n",
        "nltk.download('stopwords') # stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # required by sent_tokenize which in turn is used by word_tokenize to split text into sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwg_SZX84N5O",
        "outputId": "b58b028b-03e0-4549-f3ee-904d1b9760dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected encoding: Windows-1252\n",
            "    ID   Target                                              Tweet   Stance\n",
            "0  101  Atheism  dear lord thank u for all of ur blessings forg...  AGAINST\n",
            "1  102  Atheism  Blessed are the peacemakers, for they shall be...  AGAINST\n",
            "2  103  Atheism  I am not conformed to this world. I am transfo...  AGAINST\n",
            "3  104  Atheism  Salah should be prayed with #focus and #unders...  AGAINST\n",
            "4  105  Atheism  And stay in your houses and do not display you...  AGAINST\n"
          ]
        }
      ],
      "source": [
        "# Loading the data (Note that the file was not encoded in uft8 hence why we need to find the encoding and set it to that when using pd.read_csv)\n",
        "\n",
        "# First we detect encoding using chardet\n",
        "with open(\"Semeval.txt\", \"rb\") as f:\n",
        "    result = chardet.detect(f.read())\n",
        "Encoding = result['encoding'] # store the coding into the variable Encoding\n",
        "print(f\"Detected encoding: {Encoding}\") # It was encoded in Windows-1252\n",
        "\n",
        "# Next we read the file with the correct encoding\n",
        "data = pd.read_csv(\"Semeval.txt\", delimiter=\"\\t\", encoding=Encoding)\n",
        "\n",
        "# Let's display the top rows.\n",
        "print(data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufv7hDQM5aJZ",
        "outputId": "4a584cf6-c3ba-4ed4-c6f1-306fd51f90aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ID   Target                                              Tweet   Stance\n",
            "0  101  Atheism  dear lord thank u ur blessing forgive sin lord...  AGAINST\n",
            "1  102  Atheism  blessed peacemaker shall called child god matthew  AGAINST\n",
            "2  103  Atheism          conformed world transformed renewing mind  AGAINST\n",
            "3  104  Atheism       salah prayed warns prayer done surah almaoon  AGAINST\n",
            "4  105  Atheism       stay house display like time ignorance quran  AGAINST\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Preprocessing is a must because we are dealing with tweets which are informal in many ways. So we will\n",
        "1) Remove unnecessary characters like mentions and punctuations (there are no links because they discarded any tweets with links)\n",
        "2) Remove stopwords by using nltk\n",
        "\"\"\"\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess tweets\n",
        "def preprocess_tweet(tweet):\n",
        "    # Lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # Remove mentions\n",
        "    tweet = re.sub(r\"@\\w+\", \"\", tweet)\n",
        "\n",
        "    # Remove hashtags\n",
        "    tweet = re.sub(r\"#\\w+\", \"\", tweet)\n",
        "\n",
        "    # Remove punctuation\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
        "\n",
        "    # Remove numbers\n",
        "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
        "\n",
        "    # Tokenize\n",
        "    words = word_tokenize(tweet)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    tweet = \" \".join(tokens)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "# We apply preprocessing to the 'Tweet' column\n",
        "data['Tweet'] = data['Tweet'].apply(preprocess_tweet)\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())\n",
        "\n",
        "#Identifying the columns\n",
        "\n",
        "Id = data['ID']\n",
        "Target = data['Target']\n",
        "Tweet = data['Tweet']\n",
        "Stance = data['Stance']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xXNm234jSsTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aadbe1a-7a86-4f71-e6ef-0cdb79401998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'AGAINST': 1342, 'NONE': 741, 'FAVOR': 731})\n"
          ]
        }
      ],
      "source": [
        "# Calculating how many in each class\n",
        "from collections import Counter\n",
        "class_distribution = Counter(Stance)\n",
        "print(class_distribution)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the test data\n",
        "\"\"\"\n",
        "We will load the test data and we preprocess it.\n",
        "\"\"\"\n",
        "\n",
        "# Detecing the code for the test data\n",
        "with open(\"Testdata.txt\", \"rb\") as f:\n",
        "    result = chardet.detect(f.read())\n",
        "Encoding = result['encoding'] # store the coding into the variable Encoding\n",
        "print(f\"Detected encoding: {Encoding}\") # It was encoded in Windows-1252\n",
        "\n",
        "# Reading the test data with the right encoding\n",
        "Test_data = pd.read_csv(\"Testdata.txt\", delimiter=\"\\t\", encoding=Encoding)\n",
        "\n",
        "# Let's display the top rows.\n",
        "print(Test_data.head())\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "Test_data['Tweet'] = Test_data['Tweet'].apply(preprocess_tweet)\n",
        "\n",
        "\n",
        "#Identifying the columns for the test data\n",
        "\n",
        "Id_test = Test_data['ID']\n",
        "Target_test = Test_data['Target']\n",
        "Tweet_test = Test_data['Tweet']\n",
        "Stance_test = Test_data['Stance']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi-OG5TzcGSQ",
        "outputId": "0f4e9d63-9267-48eb-9238-94c0a4b7f6fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected encoding: ascii\n",
            "      ID   Target                                              Tweet   Stance\n",
            "0  10001  Atheism  He who exalts himself shall      be humbled; a...  AGAINST\n",
            "1  10002  Atheism  RT @prayerbullets: I remove Nehushtan -previou...  AGAINST\n",
            "2  10003  Atheism  @Brainman365 @heidtjj @BenjaminLives I have so...  AGAINST\n",
            "3  10004  Atheism  #God is utterly powerless without Human interv...  AGAINST\n",
            "4  10005  Atheism  @David_Cameron   Miracles of #Multiculturalism...  AGAINST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJPtg0CIhXhB",
        "outputId": "e2ae2fd2-ef7f-473f-8ca4-ad5c1ba2ccf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blessed peacemaker shall called child god matthew Sentence embedding: [-1.62994578e-01 -1.96897149e-01 -1.77997440e-01 -1.90302849e-01\n",
            " -9.27521512e-02  1.50632858e-01 -4.69469950e-02  1.70879111e-01\n",
            " -7.55877048e-02 -7.65702546e-01  3.39621842e-01 -5.76807261e-02\n",
            " -1.16539426e-01 -1.41729444e-01 -6.20109215e-02  1.74758703e-01\n",
            " -1.21041432e-01 -3.46834272e-01 -4.63918559e-02  6.23211376e-02\n",
            " -3.45137656e-01  3.62692028e-01  1.10021448e-02 -3.75642092e-03\n",
            " -1.57148577e-02  2.11083010e-01 -7.10457042e-02 -2.38870427e-01\n",
            "  1.19302854e-01  9.62345675e-02  1.53267458e-01  1.54039994e-01\n",
            " -2.72348616e-02  4.15671468e-02 -5.39141059e-01  1.07810304e-01\n",
            "  8.60812888e-02  7.05508664e-02 -9.25327092e-02  7.50798583e-02\n",
            "  1.30589992e-01 -7.51765594e-02 -1.60334721e-01 -1.03309579e-01\n",
            " -9.16528236e-03 -7.87018538e-02  1.07409000e-01  3.20013463e-01\n",
            "  1.70773551e-01 -2.60613978e-01  1.49713978e-01 -4.33692820e-02\n",
            " -7.97282904e-02  2.94151723e-01 -4.22611445e-01  2.48506796e-02\n",
            " -4.15757038e-02  3.74806434e-01  3.77347142e-01 -1.59863561e-01\n",
            "  2.33822420e-01  2.69814879e-01  3.53612900e-02  3.05120140e-01\n",
            " -8.99195671e-02 -2.00361416e-01 -9.33211446e-02 -1.51063293e-01\n",
            " -1.94548994e-01  3.87036413e-01 -5.55495620e-02 -2.71082193e-01\n",
            " -9.09684300e-02  8.24157074e-02  1.37831420e-01 -8.28164294e-02\n",
            " -2.47070044e-02  5.75288571e-02 -1.16687432e-01 -3.62275653e-02\n",
            " -2.50848802e-03  1.43231720e-01  4.94914353e-02  7.42317066e-02\n",
            " -2.96901673e-01  2.56782144e-01 -3.35926443e-01  4.78729546e-01\n",
            " -1.16587713e-01 -2.08811462e-02 -4.57212836e-01 -2.16817573e-01\n",
            " -7.51097128e-02 -1.26383156e-01 -3.49713594e-01 -1.26238549e-02\n",
            "  1.10443853e-01  1.67500287e-01  1.92574576e-01  1.54489860e-01\n",
            "  1.87772855e-01 -2.35513851e-01  1.35441288e-01  1.98666215e-01\n",
            "  2.40049988e-01  4.59035747e-02  1.84690982e-01  1.60097942e-01\n",
            "  1.08225130e-01 -2.95641005e-01  1.93511441e-01  2.45363697e-01\n",
            "  7.27127120e-02  1.36307567e-01  1.34577557e-01 -8.32534209e-02\n",
            "  1.34654298e-01 -3.87308568e-01  2.10685693e-02 -3.86091560e-01\n",
            "  4.08554915e-03  1.59817208e-02 -1.15233712e-01  8.27728510e-02\n",
            "  1.05817225e-02 -1.43511146e-01  2.01973125e-01  1.20067134e-01\n",
            "  6.57111481e-02 -2.45726574e-02  2.38275573e-01 -3.49758565e-01\n",
            "  1.94971725e-01  8.30107182e-02 -1.64230123e-01  7.18351379e-02\n",
            " -1.27049848e-01 -4.15435769e-02 -7.37235695e-02 -9.62157547e-03\n",
            " -1.52954292e-02  2.23048717e-01  3.65240760e-02 -9.64311361e-02\n",
            " -1.12014245e-02 -2.40611434e-01  5.98139986e-02  1.75654992e-01\n",
            "  3.73591483e-02 -1.91123143e-01 -1.76415130e-01 -3.82542908e-02\n",
            "  1.17799640e-03  3.24971452e-02  1.04044855e-01  1.98659301e-01\n",
            "  1.28552452e-01 -1.25374347e-02  1.35695711e-01 -1.70422092e-01\n",
            "  2.88926154e-01  6.58460995e-05  4.42410260e-01  1.69304267e-01\n",
            " -2.28648558e-01  2.14441456e-02  1.13664279e-02  2.42868155e-01\n",
            "  8.77814293e-02 -9.74884257e-02  5.01141436e-02 -6.33285716e-02\n",
            " -1.37074277e-01  2.18021870e-01  1.60002708e-01 -8.42713472e-03\n",
            " -1.32866576e-01  1.16802014e-01 -1.82933569e-01  2.80730307e-01\n",
            "  1.92901447e-01  2.48284247e-02  9.20474455e-02 -7.99285900e-03\n",
            "  9.42485705e-02 -2.13675141e-01 -3.43818218e-01 -1.06942855e-01\n",
            "  1.30415156e-01  6.84104264e-02  6.30844310e-02 -9.62985903e-02\n",
            "  1.21303581e-01  2.07700863e-01 -2.81847566e-01 -3.25050056e-02\n",
            "  6.58449978e-02 -1.56380862e-01  9.00047272e-02  7.55770924e-03\n",
            "  7.89497077e-01  1.11096315e-01  1.71019122e-01 -1.41604424e-01\n",
            "  1.83237284e-01 -4.39458787e-02  8.55465904e-02  1.40920863e-01\n",
            " -1.62840575e-01 -2.58443179e-03 -9.18790102e-02  1.90979987e-01\n",
            " -2.38200426e-01 -2.28799701e-01 -4.05083485e-02 -5.45762815e-02\n",
            " -4.42829989e-02  4.34190072e-02  5.92100294e-03  4.89671044e-02\n",
            " -2.74939001e-01  1.30485147e-01 -6.31764308e-02  1.55747563e-01\n",
            " -3.13404314e-02 -1.82200179e-01  2.20036298e-01 -7.14785606e-02\n",
            " -4.45724651e-02  1.84427738e-01 -1.25207156e-01 -1.68962717e-01\n",
            "  6.45031482e-02 -2.20044717e-01 -2.00042874e-01  4.03829545e-01\n",
            " -1.11941613e-01 -1.33158296e-01 -1.86082870e-01 -6.07022904e-02\n",
            " -6.92972094e-02 -6.25808316e-04 -1.11721143e-01 -1.68611422e-01\n",
            " -1.24021567e-01  1.35274157e-01 -1.87295705e-01  7.13418573e-02\n",
            " -5.44861481e-02 -9.56871510e-02 -1.15959961e-02  1.13687135e-01\n",
            " -4.82068583e-02  1.61994174e-01  4.43765700e-01  1.34185851e-01\n",
            "  1.29647419e-01 -1.88909858e-01  3.28614295e-01  1.45423725e-01\n",
            "  9.56499949e-02 -2.57860869e-01  6.79494292e-02  3.11714321e-01\n",
            " -1.97566718e-01  4.09614258e-02 -6.13088533e-02 -9.28087682e-02\n",
            " -2.61128545e-02  6.54515550e-02 -1.44343272e-01 -2.17132300e-01\n",
            "  3.53771985e-01  1.72768459e-01 -4.27455716e-02  4.49617133e-02\n",
            " -1.15236437e+00 -4.43754271e-02  3.66149992e-01  2.00294703e-01\n",
            "  4.98648621e-02 -9.61371046e-03 -2.99912870e-01  3.70854214e-02\n",
            " -8.24790969e-02  3.12107146e-01  9.17072818e-02 -7.04430416e-02\n",
            " -2.02395730e-02 -1.12728275e-01 -2.36324713e-01 -6.95420280e-02\n",
            "  3.47855762e-02  2.45430022e-02 -2.01611426e-02 -1.03641145e-01\n",
            " -6.34617135e-02  4.62815799e-02 -1.33123711e-01  9.51328799e-02]\n"
          ]
        }
      ],
      "source": [
        "#Word embedding GLOVE\n",
        "\"\"\"\n",
        "Since we be using Glove, we download it. We will be using the wikipidia one with 6B tokens with 300d. We then define a function to get the sentence embedding.\n",
        "\"\"\"\n",
        "\n",
        "glove_embeddings = {} # An empty dictionary where we will store word-vector pairs\n",
        "\n",
        "with open(\"glove.6B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to get sentence embeddings\n",
        "def get_sentence_embedding_Glove(sentence, embeddings):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = [embeddings[word] for word in words if word in embeddings]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(300)  # Return zero vector if no words are found\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "\"\"\"\n",
        "# Sometimes we run into the problem of the array having different dimension hence we can't take the mean. This function is to tackle that problem. It deos the same as getting sentence embedding\n",
        "\n",
        "\n",
        "def get_sentence_embedding_Glove(sentence, embeddings):\n",
        "    words = sentence.lower().split()\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            # If the word has multiple embeddings, take the first one\n",
        "            if isinstance(embeddings[word], list):\n",
        "                vectors.append(embeddings[word][0])  # Take the first embedding\n",
        "            else:\n",
        "                vectors.append(embeddings[word])\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(300)  # Return zero vector if no words are found\n",
        "    # Stack vectors into a 2D array before computing the mean\n",
        "    return np.mean(np.vstack(vectors), axis=0)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Example\n",
        "sentence = Tweet [1]\n",
        "sentence_embedding = get_sentence_embedding_Glove(sentence, glove_embeddings)\n",
        "print(sentence, \"Sentence embedding:\", sentence_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOKNisUWnmFc",
        "outputId": "e5ce8380-21aa-4e41-f935-063a949fc4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blessed peacemaker shall called child god matthew Sentence embedding: [-8.03718064e-03  4.33953814e-02  2.78786686e-03  3.51471454e-02\n",
            "  1.05961217e-02 -6.25265464e-02  4.02857065e-02  1.40310034e-01\n",
            "  2.87497067e-03 -2.64463462e-02 -1.20979790e-02 -5.58552481e-02\n",
            "  1.05174666e-03 -8.85106530e-03 -3.88267227e-02 -2.16841456e-02\n",
            "  1.23427557e-02  4.80516395e-03  2.81267036e-02 -9.72439069e-03\n",
            " -2.31212992e-02 -3.03364899e-02  3.82888354e-02 -4.62116936e-04\n",
            "  5.97012416e-02 -7.07284454e-03 -6.38193712e-02  5.26329596e-03\n",
            " -4.02161144e-02 -4.02505510e-02  1.14009576e-02 -3.19272727e-02\n",
            " -1.37367006e-02  4.61821957e-03  1.41318340e-03  1.89662687e-02\n",
            "  2.48746779e-02 -6.43830672e-02 -1.38941011e-03 -1.33168306e-02\n",
            " -1.88871976e-02  7.77876843e-03 -9.81551688e-03 -3.71559300e-02\n",
            "  1.16333272e-02  5.13161123e-02  1.64397471e-02  4.00561392e-02\n",
            " -2.12975051e-02  4.49087359e-02  1.50973191e-02  9.32910573e-03\n",
            " -3.97390090e-02  3.07353791e-02 -2.32112575e-02  7.74729922e-02\n",
            "  6.77889306e-03  2.07686629e-02  1.47015536e-02  3.15335742e-03\n",
            " -1.16349831e-02 -4.70772758e-03 -8.50931450e-04  8.75778962e-03\n",
            " -1.16041927e-04  1.24486899e-02  7.12494785e-03 -7.36236339e-03\n",
            " -3.88839655e-02 -6.00617612e-03 -2.86270734e-02  3.94580103e-02\n",
            "  3.88756581e-02 -4.96590436e-02 -1.79109555e-02  4.58562151e-02\n",
            " -6.26169816e-02 -1.33311420e-04 -3.37708294e-02  4.29058261e-02\n",
            " -9.26518627e-03 -6.48189113e-02  3.13691935e-03  1.05611332e-01\n",
            "  2.49528978e-02  3.22182686e-03 -2.86031198e-02  2.01196168e-02\n",
            "  5.13730943e-02  1.12809641e-02  4.24699374e-02 -2.59172600e-02\n",
            "  4.17860709e-02  1.67348348e-02  4.28440310e-02  3.42312939e-02\n",
            "  6.07577302e-02 -8.78165383e-03 -3.51175666e-03  5.54767326e-02\n",
            "  9.73493420e-03  5.65254036e-03  5.36276326e-02 -5.43193985e-03\n",
            "  1.13441553e-02 -4.50701229e-02 -1.32006630e-02  3.54102403e-02\n",
            " -3.82272266e-02  9.88511089e-03 -5.18755950e-02 -1.21597275e-02\n",
            " -6.73418259e-03  3.81160080e-02  3.93186025e-02  2.51445714e-02\n",
            " -4.16862145e-02 -2.12707091e-02  7.99118802e-02 -5.32918796e-02\n",
            "  2.69956402e-02  4.56384607e-02  2.51809079e-02 -2.84353783e-03\n",
            " -4.61587384e-02  9.19336081e-03  2.29541082e-02 -4.99122478e-02\n",
            "  1.46282231e-02  2.37166677e-02  1.87046099e-02  6.84549287e-02\n",
            " -5.58572356e-03 -5.14566414e-02  3.00450679e-02  4.35957238e-02\n",
            " -3.35046016e-02 -3.37341651e-02 -5.60565889e-02 -6.16639778e-02\n",
            "  2.28803791e-02 -5.27718589e-02 -1.40342517e-02  3.22015509e-02\n",
            "  1.78043041e-02 -3.76882851e-02 -7.82749429e-02 -2.69007627e-02\n",
            "  3.03864889e-02 -2.18981002e-02  8.79376102e-03 -1.00504771e-01\n",
            " -4.83821742e-02 -3.24349590e-02  3.98717448e-03  3.76059785e-02\n",
            " -4.22791801e-02 -2.41392013e-02  1.60950124e-02  6.62869141e-02\n",
            " -1.00573525e-02  4.72445823e-02 -5.62859103e-02  6.96118101e-02\n",
            " -3.07912547e-02  2.59512812e-02  1.48283457e-02 -3.31538767e-02\n",
            "  4.86887014e-03  8.63548890e-02 -1.10919047e-02  1.36992903e-02\n",
            "  3.21163423e-02  1.75309293e-02 -3.30963060e-02  2.62460951e-02\n",
            "  7.46664405e-03 -5.58281913e-02 -1.20915947e-02 -5.84402401e-03\n",
            " -1.27968946e-02  3.31503674e-02 -4.57443707e-02 -4.52819355e-02\n",
            " -1.84981376e-02  1.74187068e-02  4.58267815e-02  5.45713045e-02\n",
            "  3.13569866e-02 -6.30125478e-02 -1.62901729e-02 -2.22632242e-03\n",
            " -5.00008985e-02  4.88442660e-04  3.14167477e-02 -4.11945842e-02\n",
            " -8.42822716e-03 -3.68475020e-02  1.56353675e-02  3.04534961e-03\n",
            " -2.61667334e-02  2.62719989e-02 -7.38458568e-03 -2.62774471e-02\n",
            " -7.69165764e-03 -2.44731642e-02 -2.23843027e-02  2.03242190e-02\n",
            "  2.50051487e-02 -1.07535375e-02  4.61502606e-03 -2.18500774e-02\n",
            " -4.81564924e-02 -2.69551674e-04  4.15706225e-02 -5.89442812e-02\n",
            " -9.06203233e-04 -7.94050097e-02 -7.92198330e-02 -4.37454097e-02\n",
            "  5.08140288e-02 -5.55995526e-03 -3.81387472e-02 -3.07309777e-02\n",
            " -3.87280919e-02 -7.03721792e-02 -2.02718424e-03  7.46146264e-03\n",
            " -5.35181873e-02  2.13599000e-02  4.77115177e-02 -7.86487292e-03\n",
            " -2.73731817e-02  3.34153697e-02 -1.93080921e-02  1.16435708e-02\n",
            " -1.88685674e-02  1.84476990e-02  7.74839288e-03 -6.98091313e-02\n",
            "  2.80576814e-02 -2.94507053e-02 -2.79430393e-02  2.91694645e-02\n",
            "  2.88419105e-04 -7.40336925e-02 -3.63314082e-03  1.09666539e-02\n",
            "  6.07639086e-05  7.58945271e-02 -5.65584376e-03  1.79430339e-02\n",
            "  4.23865020e-02 -1.27430505e-03 -7.76217505e-02 -1.94847099e-02\n",
            "  7.63226375e-02  3.38915028e-02 -6.37466162e-02 -4.31315303e-02\n",
            "  3.40021513e-02  5.37281521e-02 -1.42082423e-02 -1.05682656e-01\n",
            " -4.67736386e-02 -3.72709823e-03  1.56269148e-02  2.09527016e-02\n",
            " -4.46289219e-02  8.48919991e-03 -5.39969504e-02 -1.28328074e-02\n",
            " -2.10086461e-02 -4.48272564e-02  8.41376185e-02  1.12987515e-02\n",
            "  4.88907136e-02  3.44412820e-03 -4.96217795e-02 -1.57901626e-02\n",
            "  3.87327746e-02  1.16844820e-02 -7.35568872e-04  1.70759298e-02\n",
            " -1.39474049e-02 -2.04897881e-03 -8.21141675e-02  5.00394069e-02\n",
            " -8.89639091e-03  5.19264303e-02  2.86600948e-03  6.16485253e-02\n",
            "  4.76114415e-02  9.35082324e-03  4.59804945e-02  5.85325435e-02\n",
            " -1.43216401e-02 -3.52906659e-02  3.21699083e-02 -3.73443426e-03]\n"
          ]
        }
      ],
      "source": [
        "#Word2Vec\n",
        "\"\"\"\n",
        "Another method we can use to turn words into a vector is using Word2Vec, which we will do here with the same dimension 300.\n",
        "\"\"\"\n",
        "\n",
        "# First we tokenize the tweets\n",
        "tokenized_tweets = [word_tokenize(tweet.lower()) for tweet in Tweet]\n",
        "\n",
        "# We will use skip-gram\n",
        "word2vec_model_SkipGram = Word2Vec(\n",
        "    sentences=tokenized_tweets,  # List of tokenized sentences\n",
        "    vector_size=300,             # Dimensionality of word embeddings (same as Glove)\n",
        "    window=5,                    # Context window size\n",
        "    min_count=1,\n",
        "    sg=1                         # 1 since we are using skip-gram (0 is for CBOW)\n",
        ")\n",
        "\n",
        "# Function to get the embedding for a sentence\n",
        "def get_sentence_embedding_word2vec(sentence, model):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)  # Return zero vector if no words are found\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "\n",
        "# Example\n",
        "sentence = Tweet [1]\n",
        "sentence_embedding = get_sentence_embedding_word2vec(sentence, word2vec_model_SkipGram)\n",
        "print(sentence, \"Sentence embedding:\", sentence_embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBtrWBQVw4hP",
        "outputId": "0cf0b8e6-ca55-42fe-8c36-8755302c1ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blessed peacemaker shall called child god matthew Sentence embedding: [ 1.20047480e-03  1.60158321e-03 -6.86998595e-04  1.14370533e-03\n",
            "  9.99160460e-04 -2.34421715e-03  2.46528583e-03  5.66589180e-03\n",
            "  1.07552922e-04 -9.59662138e-04 -5.54121914e-04 -2.21568113e-03\n",
            "  4.90455364e-04 -2.85762973e-04 -2.49680015e-03 -8.49443197e-04\n",
            "  8.27419921e-04  1.03148748e-03  1.91152038e-03  6.44701824e-04\n",
            " -1.55117153e-03 -1.18158590e-04  2.80664535e-03  6.48041780e-04\n",
            "  1.80233561e-03  1.16908726e-04 -3.62628000e-03  9.01288411e-04\n",
            " -1.53766270e-03 -2.24982179e-03  1.44706003e-03 -1.01849402e-03\n",
            "  2.11424922e-04 -7.48511811e-04  3.23263666e-04  1.28261547e-03\n",
            "  8.32915597e-04 -2.32302514e-03  7.04523060e-04  1.34790404e-04\n",
            " -1.66442525e-03 -8.54351907e-04  3.29805393e-04 -1.18393847e-03\n",
            "  2.23014009e-04  1.47570588e-03  8.22412781e-04  2.51717330e-03\n",
            " -4.18630661e-04  1.45952415e-03 -6.31384974e-05 -4.38501796e-04\n",
            " -7.59532035e-04  6.17473037e-04 -2.47908116e-04  3.31335468e-03\n",
            "  1.19512354e-03  1.20679778e-03  9.86435334e-04  1.13960930e-04\n",
            " -1.03276270e-03  1.78179209e-04 -5.60732093e-04  7.95175729e-04\n",
            "  6.17972983e-04  1.17733038e-03 -7.87013094e-04  1.19097182e-03\n",
            " -1.97162415e-04  1.89527800e-05 -1.53305579e-03  1.88102550e-03\n",
            "  1.05047016e-03 -1.01408723e-03 -5.57596286e-05  1.29416340e-03\n",
            " -1.93372217e-03  1.31635298e-03 -1.13048335e-03  2.63761240e-03\n",
            " -8.82013701e-04 -1.11375551e-03  5.09136298e-04  4.71935049e-03\n",
            "  7.78210524e-04  9.92885558e-04 -1.75732491e-03  9.84081184e-04\n",
            "  1.69356563e-03  1.34614354e-03  1.65383297e-03 -1.22776756e-03\n",
            "  3.08269984e-03  7.47013546e-04  1.15965900e-03  2.55184271e-03\n",
            "  1.94968376e-03 -6.97518699e-04 -6.90007233e-04  2.03559943e-03\n",
            "  1.50472007e-03  9.21328552e-04  8.76327686e-04  1.21390459e-03\n",
            " -6.76528143e-04 -3.53856653e-04  2.75525206e-04  2.70405406e-04\n",
            " -3.34655098e-03  3.45605513e-04 -2.07385817e-03 -1.84774306e-03\n",
            "  4.77856520e-04  1.70117593e-03  1.18047721e-03 -2.67474534e-04\n",
            " -1.78176328e-03 -5.51410310e-04  3.15955095e-03 -2.34949705e-03\n",
            "  1.25694275e-03  2.41952366e-03  7.41713739e-04  5.19999710e-04\n",
            " -1.21058815e-03  5.06751647e-04  3.00505344e-04 -1.90031424e-03\n",
            "  4.22831916e-04  1.27400446e-03  9.46250337e-04  1.24195893e-03\n",
            "  1.54009589e-03 -1.65665126e-03  5.13302439e-05  1.81099761e-03\n",
            " -9.41324630e-04 -1.38376444e-03 -1.09681289e-03 -1.76768214e-03\n",
            "  1.28990668e-03 -2.17655278e-03 -1.35148439e-04  9.61218320e-04\n",
            "  8.33562881e-05 -1.26811978e-03 -3.28077981e-03 -1.79543509e-03\n",
            "  1.63708499e-03 -2.20949623e-05 -1.50386550e-04 -5.22082951e-03\n",
            " -1.77629723e-03 -2.54320359e-04 -1.39719120e-03  1.50950148e-03\n",
            " -2.25681625e-03 -1.60658651e-03 -8.06046242e-04  3.41878226e-03\n",
            "  1.98600144e-04  2.16804096e-03 -2.85267085e-03  1.78284664e-03\n",
            " -1.83082896e-03  1.73973083e-03  1.69255982e-05 -5.62426692e-04\n",
            " -2.48459401e-04  3.72696179e-03 -2.30380843e-04  4.43237921e-04\n",
            "  8.79498140e-04  1.36272877e-03 -9.75313073e-04  1.61124079e-03\n",
            "  5.23101480e-04 -2.47885217e-03 -8.25095747e-04  3.49045324e-04\n",
            "  7.37710332e-04  1.06281799e-03 -2.76716426e-03 -9.60090023e-04\n",
            " -1.13370807e-04  2.23881565e-03  2.39264243e-03  3.28220637e-03\n",
            "  2.17370666e-03 -2.97910767e-03  1.85918598e-05  2.58117361e-04\n",
            " -2.88770045e-03 -2.53444246e-04  2.88506737e-04 -2.18198379e-03\n",
            " -2.60236178e-04 -3.88498011e-04  5.14547166e-04  9.65722196e-04\n",
            " -1.59830414e-03  6.60196587e-04 -4.59949719e-04 -8.28715798e-04\n",
            " -4.98139183e-04 -1.38233637e-03 -1.51802413e-03  1.19337963e-03\n",
            "  8.35397397e-04  5.68899966e-04  5.97081562e-05 -1.51529885e-03\n",
            " -2.01433292e-03  4.46759484e-04  2.26858677e-03 -2.87311035e-03\n",
            " -1.14672363e-03 -2.91898963e-03 -3.15373135e-03 -2.70895241e-03\n",
            "  1.93678774e-03  1.63860305e-03 -1.09613396e-03  2.19454756e-04\n",
            " -1.32285722e-03 -3.67740402e-03  1.52459205e-03  2.01574105e-04\n",
            " -3.12789343e-03 -2.06809709e-04  2.04618624e-03 -1.78575062e-03\n",
            " -1.43586367e-03  1.53404463e-03 -1.21176417e-03  6.89881970e-04\n",
            " -9.07468668e-04  7.17798423e-04  6.72262802e-04 -1.13308860e-03\n",
            "  1.49389671e-03 -1.86280420e-04 -2.75532878e-03  1.08387647e-03\n",
            " -2.00737268e-05 -3.86315910e-03 -7.50053383e-04 -6.05164269e-06\n",
            "  6.63639803e-04  2.59011216e-03  1.33443426e-03  1.10438035e-03\n",
            "  1.53024588e-03 -8.86568043e-04 -4.01732931e-03 -9.38781828e-04\n",
            "  1.37172325e-03  1.38751254e-03 -2.15489720e-03 -2.21835612e-03\n",
            "  2.00857688e-03  2.60441215e-03 -8.30846722e-04 -5.11588622e-03\n",
            " -1.62585441e-03 -4.18390351e-04  1.45624799e-03  2.15298132e-04\n",
            " -2.09825742e-03  4.29756765e-04 -2.94895493e-03 -8.37544096e-04\n",
            " -1.37022452e-03 -2.03277147e-03  2.56651314e-03 -5.02083974e-04\n",
            "  2.48130225e-03  7.03159312e-04 -1.68177986e-03 -1.16032653e-03\n",
            "  3.23544897e-04  1.60048730e-04  6.96987845e-05  6.22732739e-04\n",
            " -1.84544630e-03 -3.24657914e-04 -2.37233960e-03  1.05826941e-03\n",
            "  1.86091915e-04  2.15443736e-03  1.16118614e-03  2.27411720e-03\n",
            "  2.35700072e-03  3.34255106e-04  4.13781498e-03  2.40868842e-03\n",
            "  5.89138828e-04 -2.47639790e-03  1.48472073e-03 -6.88457221e-04]\n"
          ]
        }
      ],
      "source": [
        "#CBOW\n",
        "\n",
        "word2vec_model_CBOW = Word2Vec(\n",
        "    sentences=tokenized_tweets,  # List of tokenized sentences\n",
        "    vector_size=300,             # Dimensionality of word embeddings (same as Glove)\n",
        "    window=5,                    # Context window size\n",
        "    min_count=1,\n",
        "    sg=0                         # 0 for CBOW\n",
        ")\n",
        "\n",
        "# Example\n",
        "sentence = Tweet [1]\n",
        "sentence_embedding = get_sentence_embedding_word2vec(sentence, word2vec_model_CBOW)\n",
        "print(sentence, \"Sentence embedding:\", sentence_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(Tweet)\n",
        "print(X_train_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC6JRVR2jbmo",
        "outputId": "c368018f-0c4c-4ee9-cd0e-2fccf1d5ec93"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 4587)\t0.14814727000362926\n",
            "  (0, 11900)\t0.29218047222408566\n",
            "  (0, 19813)\t0.13996326252995958\n",
            "  (0, 21040)\t0.154150985775193\n",
            "  (0, 2103)\t0.18377393191228916\n",
            "  (0, 7451)\t0.1941940625335076\n",
            "  (0, 18313)\t0.16819672410550798\n",
            "  (0, 8085)\t0.13633765482086307\n",
            "  (0, 19167)\t0.18845945083578208\n",
            "  (0, 5928)\t0.17981238210928824\n",
            "  (0, 2517)\t0.17638073157917766\n",
            "  (0, 4488)\t0.11955933102559835\n",
            "  (0, 482)\t0.1941940625335076\n",
            "  (0, 4593)\t0.21200739348783754\n",
            "  (0, 11923)\t0.21200739348783754\n",
            "  (0, 19839)\t0.21200739348783754\n",
            "  (0, 21042)\t0.21200739348783754\n",
            "  (0, 2104)\t0.21200739348783754\n",
            "  (0, 7452)\t0.20158726286661913\n",
            "  (0, 18319)\t0.21200739348783754\n",
            "  (0, 11908)\t0.21200739348783754\n",
            "  (0, 8116)\t0.21200739348783754\n",
            "  (0, 19168)\t0.21200739348783754\n",
            "  (0, 5929)\t0.21200739348783754\n",
            "  (0, 2519)\t0.21200739348783754\n",
            "  :\t:\n",
            "  (2812, 6047)\t0.23871107212935166\n",
            "  (2812, 16900)\t0.3124000531979786\n",
            "  (2812, 2256)\t0.3124000531979786\n",
            "  (2812, 21671)\t0.3124000531979786\n",
            "  (2812, 16981)\t0.3124000531979786\n",
            "  (2812, 22739)\t0.3124000531979786\n",
            "  (2813, 21982)\t0.1698083307094832\n",
            "  (2813, 19029)\t0.1521228591935119\n",
            "  (2813, 198)\t0.20501655959955428\n",
            "  (2813, 11106)\t0.16654638070878708\n",
            "  (2813, 438)\t0.20900534669172846\n",
            "  (2813, 1510)\t0.22572192687276785\n",
            "  (2813, 16672)\t0.23431543072749242\n",
            "  (2813, 17632)\t0.2464272941459814\n",
            "  (2813, 11259)\t0.2464272941459814\n",
            "  (2813, 5453)\t0.2464272941459814\n",
            "  (2813, 17633)\t0.2464272941459814\n",
            "  (2813, 11260)\t0.2464272941459814\n",
            "  (2813, 5454)\t0.2464272941459814\n",
            "  (2813, 21983)\t0.2464272941459814\n",
            "  (2813, 441)\t0.2464272941459814\n",
            "  (2813, 19062)\t0.2464272941459814\n",
            "  (2813, 16674)\t0.2464272941459814\n",
            "  (2813, 11108)\t0.2464272941459814\n",
            "  (2813, 1511)\t0.2464272941459814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOVwbHHExWXB",
        "outputId": "6424ba2a-67d7-4d63-8153-1f84394c47a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.105629324913025\n",
            "Epoch 2, Loss: 1.1053701639175415\n",
            "Epoch 3, Loss: 1.1051167249679565\n",
            "Epoch 4, Loss: 1.1048682928085327\n",
            "Epoch 5, Loss: 1.1046251058578491\n",
            "Epoch 6, Loss: 1.104386806488037\n",
            "Epoch 7, Loss: 1.104153037071228\n",
            "Epoch 8, Loss: 1.103924036026001\n",
            "Epoch 9, Loss: 1.1036990880966187\n",
            "Epoch 10, Loss: 1.1034783124923706\n",
            "Epoch 1, Loss: 1.0985606908798218\n",
            "Epoch 2, Loss: 1.0985573530197144\n",
            "Epoch 3, Loss: 1.0985541343688965\n",
            "Epoch 4, Loss: 1.0985511541366577\n",
            "Epoch 5, Loss: 1.0985478162765503\n",
            "Epoch 6, Loss: 1.0985445976257324\n",
            "Epoch 7, Loss: 1.098541259765625\n",
            "Epoch 8, Loss: 1.0985380411148071\n",
            "Epoch 9, Loss: 1.0985349416732788\n",
            "Epoch 10, Loss: 1.098531723022461\n",
            "Epoch 1, Loss: 1.0991383790969849\n",
            "Epoch 2, Loss: 1.0991345643997192\n",
            "Epoch 3, Loss: 1.0991308689117432\n",
            "Epoch 4, Loss: 1.0991270542144775\n",
            "Epoch 5, Loss: 1.0991233587265015\n",
            "Epoch 6, Loss: 1.0991196632385254\n",
            "Epoch 7, Loss: 1.0991159677505493\n",
            "Epoch 8, Loss: 1.0991121530532837\n",
            "Epoch 9, Loss: 1.0991084575653076\n",
            "Epoch 10, Loss: 1.0991050004959106\n",
            "Epoch 1, Loss: 1.098558783531189\n",
            "Epoch 2, Loss: 1.0985547304153442\n",
            "Epoch 3, Loss: 1.098550796508789\n",
            "Epoch 4, Loss: 1.0985468626022339\n",
            "Epoch 5, Loss: 1.0985429286956787\n",
            "Epoch 6, Loss: 1.098538875579834\n",
            "Epoch 7, Loss: 1.0985349416732788\n",
            "Epoch 8, Loss: 1.0985310077667236\n",
            "Epoch 9, Loss: 1.0985270738601685\n",
            "Epoch 10, Loss: 1.0985231399536133\n"
          ]
        }
      ],
      "source": [
        "# Perceptron classifier\n",
        "\n",
        "\"\"\"\n",
        "Next we will define a perceptron model and train it using the training data. But first we need to convert the data to PyTorch tensors.\n",
        "So we need to define the X and Y, with X being the tweet and Y being the stance. And since we have 3 types of word embeddings, we will do them sepereately.\n",
        "\"\"\"\n",
        "\n",
        "X_train_Glove = np.array([get_sentence_embedding_Glove(tweet, glove_embeddings) for tweet in Tweet])\n",
        "X_train_word2vec_SkipGram = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_SkipGram) for tweet in Tweet])\n",
        "X_train_word2vec_CBOW = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_CBOW) for tweet in Tweet])\n",
        "X_train_tfidf = vectorizer.transform(Tweet)\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray() #TfidfVectorizer returns a sparse matrix and pytorch tensor requires an array\n",
        "\n",
        "\n",
        "\n",
        "# Converting data to PyTorch tensors\n",
        "X_train_tensor_Glove = torch.tensor(X_train_Glove, dtype=torch.float32)\n",
        "X_train_tensor_word2vec_SkipGram = torch.tensor(X_train_word2vec_SkipGram, dtype=torch.float32)\n",
        "X_train_tensor_word2vec_CBOW = torch.tensor(X_train_word2vec_CBOW, dtype=torch.float32)\n",
        "X_train_tensor_tfidf = torch.tensor(X_train_tfidf_dense, dtype=torch.float32)\n",
        "\n",
        "y_train_tensor = torch.tensor(Stance.factorize()[0], dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "# Define the Perceptron model\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = 300  # Word2Vec and Glove embedding size\n",
        "input_dim_tfidf = X_train_tensor_tfidf.shape[1]  # Number of features (TF-IDF dimensions)\n",
        "output_dim = 3   # Number of stance classes (FAVOR, AGAINST, NONE)\n",
        "model_Glove = Perceptron(input_dim, output_dim)\n",
        "model_word2vec_SkipGram = Perceptron(input_dim, output_dim)\n",
        "model_word2vec_CBOW = Perceptron(input_dim, output_dim)\n",
        "model_tfidf = Perceptron(input_dim_tfidf, output_dim)\n",
        "\n",
        "\n",
        "# Adjusting weights\n",
        "weights = torch.tensor([1/0.48, 1/0.26, 1/0.26]) # AGAINST : 0.48, NONE : 0.26, FAVOR : 0.26\n",
        "\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weights) # Used for multi-class classification\n",
        "\n",
        "#Glove optimiser SGD\n",
        "optimizer_Glove = optim.SGD(model_Glove.parameters(), lr=0.01)\n",
        "\n",
        "#Skipgram optimiser SGD\n",
        "optimizer_SkipGram = optim.SGD(model_word2vec_SkipGram.parameters(), lr=0.01)\n",
        "\n",
        "#CBOW optimiser SGD\n",
        "optimizer_CBOW = optim.SGD(model_word2vec_CBOW.parameters(), lr=0.01)\n",
        "\n",
        "# TF-IDF optimiser SGD\n",
        "optimizer_tfidf = optim.SGD(model_tfidf.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Glove optimiser adam\n",
        "optimizer_Glove = optim.Adam(model_Glove.parameters(), lr=0.001)\n",
        "\n",
        "#Skipgram optimiser adam\n",
        "optimizer_SkipGram = optim.Adam(model_word2vec_SkipGram.parameters(), lr=0.001)\n",
        "\n",
        "#CBOW opitmiser adam\n",
        "optimizer_CBOW = optim.Adam(model_word2vec_CBOW.parameters(), lr=0.001)\n",
        "\n",
        "#TF-IDF optimiser adam\n",
        "optimizer_tfidif = optim.Adam(model_tfidf.parameters(), lr=0.001)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Train the model for Glove\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer_Glove.zero_grad() # optimise for gradient 0\n",
        "    outputs = model_Glove(X_train_tensor_Glove) # predict the output based on our model\n",
        "    loss = criterion(outputs, y_train_tensor) # calculate the loss\n",
        "    loss.backward() # Compute the gradient\n",
        "    optimizer_Glove.step() # update the weights\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Train the model for SkipGram\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer_SkipGram.zero_grad() # optimise for gradient 0\n",
        "    outputs = model_word2vec_SkipGram(X_train_tensor_word2vec_SkipGram) # predict the output based on our model\n",
        "    loss = criterion(outputs, y_train_tensor) # calculate the loss\n",
        "    loss.backward() # Compute the gradient\n",
        "    optimizer_SkipGram.step() # update the weights\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Train the model for CBOW\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer_CBOW.zero_grad() # optimise for gradient 0\n",
        "    outputs = model_word2vec_CBOW(X_train_tensor_word2vec_CBOW) # predict the output based on our model\n",
        "    loss = criterion(outputs, y_train_tensor) # calculate the loss\n",
        "    loss.backward() # Compute the gradient\n",
        "    optimizer_CBOW.step() # update the weights\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Train the model for TF-IDF\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer_tfidf.zero_grad() # optimise for gradient 0\n",
        "    outputs = model_tfidf(X_train_tensor_tfidf) # predict the output based on our model\n",
        "    loss = criterion(outputs, y_train_tensor) # calculate the loss\n",
        "    loss.backward() # Compute the gradient\n",
        "    optimizer_tfidf.step() # update the weights\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yte0SBzc650G",
        "outputId": "ef9ffa94-f76b-4190-deb1-2f7fbe1f2df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'FAVOR': 579, 'NONE': 553, 'AGAINST': 117})\n",
            "Counter({'AGAINST': 1249})\n",
            "Counter({'FAVOR': 1249})\n",
            "Counter({'NONE': 507, 'AGAINST': 492, 'FAVOR': 250})\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.56      0.09      0.16       715\n",
            "       FAVOR       0.27      0.52      0.36       304\n",
            "        NONE       0.19      0.45      0.26       230\n",
            "\n",
            "    accuracy                           0.26      1249\n",
            "   macro avg       0.34      0.35      0.26      1249\n",
            "weighted avg       0.42      0.26      0.23      1249\n",
            "\n",
            "Micro-average F1 score: 0.2618094475580464\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.57      1.00      0.73       715\n",
            "       FAVOR       0.00      0.00      0.00       304\n",
            "        NONE       0.00      0.00      0.00       230\n",
            "\n",
            "    accuracy                           0.57      1249\n",
            "   macro avg       0.19      0.33      0.24      1249\n",
            "weighted avg       0.33      0.57      0.42      1249\n",
            "\n",
            "Micro-average F1 score: 0.5724579663730984\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.00      0.00      0.00       715\n",
            "       FAVOR       0.24      1.00      0.39       304\n",
            "        NONE       0.00      0.00      0.00       230\n",
            "\n",
            "    accuracy                           0.24      1249\n",
            "   macro avg       0.08      0.33      0.13      1249\n",
            "weighted avg       0.06      0.24      0.10      1249\n",
            "\n",
            "Micro-average F1 score: 0.2433947157726181\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.58      0.40      0.48       715\n",
            "       FAVOR       0.21      0.17      0.19       304\n",
            "        NONE       0.18      0.39      0.24       230\n",
            "\n",
            "    accuracy                           0.34      1249\n",
            "   macro avg       0.32      0.32      0.30      1249\n",
            "weighted avg       0.42      0.34      0.36      1249\n",
            "\n",
            "Micro-average F1 score: 0.3434747798238591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Evaluation of Perceptron\n",
        "\n",
        "\"\"\"\n",
        "First we convert the data into word embedding and then convert it into tensors.\n",
        "We then make predictions and then convert them into labels, i.e. stances.\n",
        "We will then use Micro-average F1 score.\n",
        "\"\"\"\n",
        "\n",
        "X_test_Glove = np.array([get_sentence_embedding_Glove(tweet, glove_embeddings) for tweet in Tweet_test])\n",
        "X_test_word2vec_SkipGram = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_SkipGram) for tweet in Tweet_test])\n",
        "X_test_word2vec_CBOW = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_CBOW) for tweet in Tweet_test])\n",
        "X_test_tfidf = vectorizer.transform(Tweet_test)\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray() #TfidfVectorizer returns a sparse matrix and pytorch tensor requires an array\n",
        "\n",
        "\n",
        "# Converting data to PyTorch tensors\n",
        "X_test_tensor_Glove = torch.tensor(X_test_Glove, dtype=torch.float32)\n",
        "X_test_tensor_word2vec_SkipGram = torch.tensor(X_test_word2vec_SkipGram, dtype=torch.float32)\n",
        "X_test_tensor_word2vec_CBOW = torch.tensor(X_test_word2vec_CBOW, dtype=torch.float32)\n",
        "X_test_tensor_tfidf = torch.tensor(X_test_tfidf_dense, dtype=torch.float32)\n",
        "\n",
        "\n",
        "y_test_tensor = torch.tensor(Stance_test.factorize()[0], dtype=torch.long)\n",
        "\n",
        "# Make predictions\n",
        "#Glove\n",
        "with torch.no_grad():\n",
        "    outputs = model_Glove(X_test_tensor_Glove)\n",
        "    _, predicted_Glove = torch.max(outputs, 1)\n",
        "\n",
        "#SkipGram\n",
        "with torch.no_grad():\n",
        "    outputs = model_word2vec_SkipGram(X_test_tensor_word2vec_SkipGram)\n",
        "    _, predicted_SkipGram = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "#CBOW\n",
        "with torch.no_grad():\n",
        "    outputs = model_word2vec_CBOW(X_test_tensor_word2vec_CBOW)\n",
        "    _, predicted_CBOW = torch.max(outputs, 1)\n",
        "\n",
        "#TF-IDF\n",
        "with torch.no_grad():\n",
        "    outputs = model_tfidf(X_test_tensor_tfidf)\n",
        "    _, predicted_tfidf = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels_Glove = [[\"FAVOR\", \"AGAINST\", \"NONE\"][i] for i in predicted_Glove]\n",
        "predicted_labels_SkipGram = [[\"FAVOR\", \"AGAINST\", \"NONE\"][i] for i in predicted_SkipGram]\n",
        "predicted_labels_CBOW = [[\"FAVOR\", \"AGAINST\", \"NONE\"][i] for i in predicted_CBOW]\n",
        "predicted_labels_tfidf = [[\"FAVOR\", \"AGAINST\", \"NONE\"][i] for i in predicted_tfidf]\n",
        "\n",
        "\n",
        "print(Counter(predicted_labels_Glove))\n",
        "print(Counter(predicted_labels_SkipGram))\n",
        "print(Counter(predicted_labels_CBOW))\n",
        "print(Counter(predicted_labels_tfidf))\n",
        "\n",
        "# Calculate F1 score For each\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "print(classification_report(Stance_test, predicted_labels_Glove))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, predicted_labels_Glove, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, predicted_labels_Glove, average='micro'))\n",
        "print(classification_report(Stance_test, predicted_labels_SkipGram))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, predicted_labels_SkipGram, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, predicted_labels_SkipGram, average='micro'))\n",
        "print(classification_report(Stance_test, predicted_labels_CBOW))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, predicted_labels_CBOW, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, predicted_labels_CBOW, average='micro'))\n",
        "print(classification_report(Stance_test, predicted_labels_tfidf))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, predicted_labels_tfidf, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, predicted_labels_tfidf, average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlm0NyaSLErY",
        "outputId": "393000aa-beee-4969-df30-fb5991cf3f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.74      0.55      0.63       715\n",
            "       FAVOR       0.37      0.49      0.42       304\n",
            "        NONE       0.31      0.43      0.36       230\n",
            "\n",
            "    accuracy                           0.51      1249\n",
            "   macro avg       0.47      0.49      0.47      1249\n",
            "weighted avg       0.57      0.51      0.53      1249\n",
            "\n",
            "Micro-average F1 score: 0.510808646917534\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.57      0.23      0.33       715\n",
            "       FAVOR       0.24      0.38      0.29       304\n",
            "        NONE       0.22      0.47      0.30       230\n",
            "\n",
            "    accuracy                           0.31      1249\n",
            "   macro avg       0.35      0.36      0.31      1249\n",
            "weighted avg       0.43      0.31      0.31      1249\n",
            "\n",
            "Micro-average F1 score: 0.3098478783026421\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.60      0.32      0.42       715\n",
            "       FAVOR       0.24      0.33      0.28       304\n",
            "        NONE       0.25      0.50      0.34       230\n",
            "\n",
            "    accuracy                           0.36      1249\n",
            "   macro avg       0.37      0.38      0.34      1249\n",
            "weighted avg       0.45      0.36      0.37      1249\n",
            "\n",
            "Micro-average F1 score: 0.35628502802241796\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.59      0.99      0.74       715\n",
            "       FAVOR       0.80      0.12      0.20       304\n",
            "        NONE       0.67      0.03      0.07       230\n",
            "\n",
            "    accuracy                           0.60      1249\n",
            "   macro avg       0.69      0.38      0.34      1249\n",
            "weighted avg       0.66      0.60      0.49      1249\n",
            "\n",
            "Micro-average F1 score: 0.6012810248198559\n"
          ]
        }
      ],
      "source": [
        "# 2nd ML method: Naive Bayes classification\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Train Naive Bayes classifier for tfidf\n",
        "nb_classifier_tfidf = MultinomialNB()\n",
        "nb_classifier_tfidf.fit(X_train_tfidf, Stance)\n",
        "\n",
        "# Glove\n",
        "nb_classifier_Glove = GaussianNB()\n",
        "nb_classifier_Glove.fit(X_train_Glove, Stance)\n",
        "\n",
        "# Skipgram\n",
        "nb_classifier_skipgram = GaussianNB()\n",
        "nb_classifier_skipgram.fit(X_train_word2vec_SkipGram, Stance)\n",
        "\n",
        "# CBOW\n",
        "nb_classifier_cbow = GaussianNB()\n",
        "nb_classifier_cbow.fit(X_train_word2vec_CBOW, Stance)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tfidf = nb_classifier_tfidf.predict(X_test_tfidf)\n",
        "y_pred_Glove = nb_classifier_Glove.predict(X_test_Glove)\n",
        "y_pred_skipgram = nb_classifier_skipgram.predict(X_test_word2vec_SkipGram)\n",
        "y_pred_cbow = nb_classifier_cbow.predict(X_test_word2vec_CBOW)\n",
        "\n",
        "\n",
        "# Evaluate performance\n",
        "print(classification_report(Stance_test, y_pred_Glove))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_Glove, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_Glove, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_skipgram))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_skipgram, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_skipgram, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_cbow))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_cbow, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_cbow, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_tfidf))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_tfidf, average='micro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_tfidf, average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHLcHV-8mvjA",
        "outputId": "780eb446-988a-4b83-b73e-5adafeb95363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.70      0.73      0.71       715\n",
            "       FAVOR       0.49      0.40      0.44       304\n",
            "        NONE       0.35      0.40      0.37       230\n",
            "\n",
            "    accuracy                           0.59      1249\n",
            "   macro avg       0.51      0.51      0.51      1249\n",
            "weighted avg       0.59      0.59      0.58      1249\n",
            "\n",
            "Micro-average F1 score: 0.5860688550840673\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.57      1.00      0.73       715\n",
            "       FAVOR       0.00      0.00      0.00       304\n",
            "        NONE       0.00      0.00      0.00       230\n",
            "\n",
            "    accuracy                           0.57      1249\n",
            "   macro avg       0.19      0.33      0.24      1249\n",
            "weighted avg       0.33      0.57      0.42      1249\n",
            "\n",
            "Micro-average F1 score: 0.5724579663730984\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.57      1.00      0.73       715\n",
            "       FAVOR       0.00      0.00      0.00       304\n",
            "        NONE       0.00      0.00      0.00       230\n",
            "\n",
            "    accuracy                           0.57      1249\n",
            "   macro avg       0.19      0.33      0.24      1249\n",
            "weighted avg       0.33      0.57      0.42      1249\n",
            "\n",
            "Micro-average F1 score: 0.5724579663730984\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     AGAINST       0.66      0.75      0.71       715\n",
            "       FAVOR       0.47      0.37      0.41       304\n",
            "        NONE       0.42      0.37      0.39       230\n",
            "\n",
            "    accuracy                           0.59      1249\n",
            "   macro avg       0.52      0.50      0.50      1249\n",
            "weighted avg       0.57      0.59      0.58      1249\n",
            "\n",
            "Micro-average F1 score: 0.588470776621297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "#svm\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Train SVM classifier\n",
        "\n",
        "#TF-IDF\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_tfidf, Stance)\n",
        "\n",
        "#Glove\n",
        "svm_classifier_Glove = SVC(kernel='linear')\n",
        "svm_classifier_Glove.fit(X_train_Glove, Stance)\n",
        "\n",
        "#Skipgram\n",
        "svm_classifier_skipgram = SVC(kernel='linear')\n",
        "svm_classifier_skipgram.fit(X_train_word2vec_SkipGram, Stance)\n",
        "\n",
        "#CBOW\n",
        "svm_classifier_cbow = SVC(kernel='linear')\n",
        "svm_classifier_cbow.fit(X_train_word2vec_CBOW, Stance)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tfidf_svm = svm_classifier.predict(X_test_tfidf)\n",
        "y_pred_Glove_svm = svm_classifier_Glove.predict(X_test_Glove)\n",
        "y_pred_skipgram_svm = svm_classifier_skipgram.predict(X_test_word2vec_SkipGram)\n",
        "y_pred_cbow_svm = svm_classifier_cbow.predict(X_test_word2vec_CBOW)\n",
        "\n",
        "# Evaluate performance\n",
        "print(classification_report(Stance_test, y_pred_Glove_svm))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_Glove_svm, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_Glove_svm, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_skipgram_svm))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_skipgram_svm, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_skipgram_svm, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_cbow_svm))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_cbow_svm, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_cbow_svm, average='micro'))\n",
        "print(classification_report(Stance_test, y_pred_tfidf_svm))\n",
        "#print(\"Macro-average F1 score:\", f1_score(Stance_test, y_pred_tfidf_svm, average='macro'))\n",
        "print(\"Micro-average F1 score:\", f1_score(Stance_test, y_pred_tfidf_svm, average='micro'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}