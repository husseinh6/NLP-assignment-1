# -*- coding: utf-8 -*-
"""Assignment 1 NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b9nuL6SOCGcfZf4iUJ5MInvTIilsbGsh
"""

#Importing all the necessary packages

import pandas as pd # For data reading
import chardet # For detecting the correct encoding of the data
import re # For preprocessing
from nltk.corpus import stopwords # For removing stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import numpy as np
from gensim.models import Word2Vec # For Skip-gram and CBOW
import torch
import torch.nn as nn # for perceptron classifier
import torch.optim as optim # to optimise the loss function


# Downloading
import nltk
nltk.download('punkt') #tokenizer
nltk.download('stopwords') # stopwords
nltk.download('wordnet')
nltk.download('punkt_tab') # required by sent_tokenize which in turn is used by word_tokenize to split text into sentences

# Loading the data (Note that the file was not encoded in uft8 hence why we need to find the encoding and set it to that when using pd.read_csv)

# First we detect encoding using chardet
with open("Semeval.txt", "rb") as f:
    result = chardet.detect(f.read())
Encoding = result['encoding'] # store the coding into the variable Encoding
print(f"Detected encoding: {Encoding}") # It was encoded in Windows-1252

# Next we read the file with the correct encoding
data = pd.read_csv("Semeval.txt", delimiter="\t", encoding=Encoding)

# Let's display the top rows.
print(data.head())

"""
Preprocessing is a must because we are dealing with tweets which are informal in many ways. So we will
1) Remove unnecessary characters like mentions and punctuations (there are no links because they discarded any tweets with links)
2) Remove stopwords by using nltk
"""

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Function to preprocess tweets
def preprocess_tweet(tweet):
    # Lowercase
    tweet = tweet.lower()

    # Remove mentions
    tweet = re.sub(r"@\w+", "", tweet)

    # Remove hashtags
    tweet = re.sub(r"#\w+", "", tweet)

    # Remove punctuation
    tweet = re.sub(r"[^\w\s]", "", tweet)

    # Remove numbers
    tweet = re.sub(r"\d+", "", tweet)

    # Tokenize
    words = word_tokenize(tweet)

    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]

    # Join tokens back into a string
    tweet = " ".join(tokens)

    return tweet

# We apply preprocessing to the 'Tweet' column
data['Tweet'] = data['Tweet'].apply(preprocess_tweet)

# Display the first few rows
print(data.head())

#Identifying the columns

Id = data['ID']
Target = data['Target']
Tweet = data['Tweet']
Stance = data['Stance']

# Calculating how many in each class
from collections import Counter
class_distribution = Counter(Stance)
print(class_distribution)

# Loading the test data
"""
We will load the test data and we preprocess it.
"""

# Detecing the code for the test data
with open("Testdata.txt", "rb") as f:
    result = chardet.detect(f.read())
Encoding = result['encoding'] # store the coding into the variable Encoding
print(f"Detected encoding: {Encoding}") # It was encoded in Windows-1252

# Reading the test data with the right encoding
Test_data = pd.read_csv("Testdata.txt", delimiter="\t", encoding=Encoding)

# Let's display the top rows.
print(Test_data.head())


# Preprocessing
Test_data['Tweet'] = Test_data['Tweet'].apply(preprocess_tweet)


#Identifying the columns for the test data

Id_test = Test_data['ID']
Target_test = Test_data['Target']
Tweet_test = Test_data['Tweet']
Stance_test = Test_data['Stance']

#Word embedding GLOVE
"""
Since we be using Glove, we download it. We will be using the wikipidia one with 6B tokens with 300d. We then define a function to get the sentence embedding.
"""

glove_embeddings = {} # An empty dictionary where we will store word-vector pairs

with open("glove.6B.300d.txt", "r", encoding="utf-8") as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.array(values[1:], dtype='float32')
        glove_embeddings[word] = vector




# Function to get sentence embeddings
def get_sentence_embedding_Glove(sentence, embeddings):
    words = sentence.lower().split()
    vectors = [embeddings[word] for word in words if word in embeddings]
    if len(vectors) == 0:
        return np.zeros(300)  # Return zero vector if no words are found
    return np.mean(vectors, axis=0)

"""
# Sometimes we run into the problem of the array having different dimension hence we can't take the mean. This function is to tackle that problem. It deos the same as getting sentence embedding


def get_sentence_embedding_Glove(sentence, embeddings):
    words = sentence.lower().split()
    vectors = []
    for word in words:
        if word in embeddings:
            # If the word has multiple embeddings, take the first one
            if isinstance(embeddings[word], list):
                vectors.append(embeddings[word][0])  # Take the first embedding
            else:
                vectors.append(embeddings[word])
    if len(vectors) == 0:
        return np.zeros(300)  # Return zero vector if no words are found
    # Stack vectors into a 2D array before computing the mean
    return np.mean(np.vstack(vectors), axis=0)


"""



# Example
sentence = Tweet [1]
sentence_embedding = get_sentence_embedding_Glove(sentence, glove_embeddings)
print(sentence, "Sentence embedding:", sentence_embedding)

#Word2Vec
"""
Another method we can use to turn words into a vector is using Word2Vec, which we will do here with the same dimension 300.
"""

# First we tokenize the tweets
tokenized_tweets = [word_tokenize(tweet.lower()) for tweet in Tweet]

# We will use skip-gram
word2vec_model_SkipGram = Word2Vec(
    sentences=tokenized_tweets,  # List of tokenized sentences
    vector_size=300,             # Dimensionality of word embeddings (same as Glove)
    window=5,                    # Context window size
    min_count=1,
    sg=1                         # 1 since we are using skip-gram (0 is for CBOW)
)

# Function to get the embedding for a sentence
def get_sentence_embedding_word2vec(sentence, model):
    words = word_tokenize(sentence.lower())
    vectors = [model.wv[word] for word in words if word in model.wv]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)  # Return zero vector if no words are found
    return np.mean(vectors, axis=0)


# Example
sentence = Tweet [1]
sentence_embedding = get_sentence_embedding_word2vec(sentence, word2vec_model_SkipGram)
print(sentence, "Sentence embedding:", sentence_embedding)

#CBOW

word2vec_model_CBOW = Word2Vec(
    sentences=tokenized_tweets,  # List of tokenized sentences
    vector_size=300,             # Dimensionality of word embeddings (same as Glove)
    window=5,                    # Context window size
    min_count=1,
    sg=0                         # 0 for CBOW
)

# Example
sentence = Tweet [1]
sentence_embedding = get_sentence_embedding_word2vec(sentence, word2vec_model_CBOW)
print(sentence, "Sentence embedding:", sentence_embedding)

# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(Tweet)
print(X_train_tfidf)

# Perceptron classifier

"""
Next we will define a perceptron model and train it using the training data. But first we need to convert the data to PyTorch tensors.
So we need to define the X and Y, with X being the tweet and Y being the stance. And since we have 3 types of word embeddings, we will do them sepereately.
"""

X_train_Glove = np.array([get_sentence_embedding_Glove(tweet, glove_embeddings) for tweet in Tweet])
X_train_word2vec_SkipGram = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_SkipGram) for tweet in Tweet])
X_train_word2vec_CBOW = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_CBOW) for tweet in Tweet])
X_train_tfidf = vectorizer.transform(Tweet)
X_train_tfidf_dense = X_train_tfidf.toarray() #TfidfVectorizer returns a sparse matrix and pytorch tensor requires an array



# Converting data to PyTorch tensors
X_train_tensor_Glove = torch.tensor(X_train_Glove, dtype=torch.float32)
X_train_tensor_word2vec_SkipGram = torch.tensor(X_train_word2vec_SkipGram, dtype=torch.float32)
X_train_tensor_word2vec_CBOW = torch.tensor(X_train_word2vec_CBOW, dtype=torch.float32)
X_train_tensor_tfidf = torch.tensor(X_train_tfidf_dense, dtype=torch.float32)

y_train_tensor = torch.tensor(Stance.factorize()[0], dtype=torch.long)



# Define the Perceptron model
class Perceptron(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(Perceptron, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.fc(x)

# Initialize the model
input_dim = 300  # Word2Vec and Glove embedding size
input_dim_tfidf = X_train_tensor_tfidf.shape[1]  # Number of features (TF-IDF dimensions)
output_dim = 3   # Number of stance classes (FAVOR, AGAINST, NONE)
model_Glove = Perceptron(input_dim, output_dim)
model_word2vec_SkipGram = Perceptron(input_dim, output_dim)
model_word2vec_CBOW = Perceptron(input_dim, output_dim)
model_tfidf = Perceptron(input_dim_tfidf, output_dim)


# Adjusting weights
weights = torch.tensor([1/0.48, 1/0.26, 1/0.26]) # AGAINST : 0.48, NONE : 0.26, FAVOR : 0.26



# Define loss function and optimizer
criterion = nn.CrossEntropyLoss(weights) # Used for multi-class classification

#Glove optimiser SGD
optimizer_Glove = optim.SGD(model_Glove.parameters(), lr=0.01)

#Skipgram optimiser SGD
optimizer_SkipGram = optim.SGD(model_word2vec_SkipGram.parameters(), lr=0.01)

#CBOW optimiser SGD
optimizer_CBOW = optim.SGD(model_word2vec_CBOW.parameters(), lr=0.01)

# TF-IDF optimiser SGD
optimizer_tfidf = optim.SGD(model_tfidf.parameters(), lr=0.01)



"""

#Glove optimiser adam
optimizer_Glove = optim.Adam(model_Glove.parameters(), lr=0.001)

#Skipgram optimiser adam
optimizer_SkipGram = optim.Adam(model_word2vec_SkipGram.parameters(), lr=0.001)

#CBOW opitmiser adam
optimizer_CBOW = optim.Adam(model_word2vec_CBOW.parameters(), lr=0.001)

#TF-IDF optimiser adam
optimizer_tfidif = optim.Adam(model_tfidf.parameters(), lr=0.001)

"""


# Train the model for Glove
for epoch in range(10):  # Number of epochs
    optimizer_Glove.zero_grad() # optimise for gradient 0
    outputs = model_Glove(X_train_tensor_Glove) # predict the output based on our model
    loss = criterion(outputs, y_train_tensor) # calculate the loss
    loss.backward() # Compute the gradient
    optimizer_Glove.step() # update the weights
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Train the model for SkipGram
for epoch in range(10):  # Number of epochs
    optimizer_SkipGram.zero_grad() # optimise for gradient 0
    outputs = model_word2vec_SkipGram(X_train_tensor_word2vec_SkipGram) # predict the output based on our model
    loss = criterion(outputs, y_train_tensor) # calculate the loss
    loss.backward() # Compute the gradient
    optimizer_SkipGram.step() # update the weights
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Train the model for CBOW
for epoch in range(10):  # Number of epochs
    optimizer_CBOW.zero_grad() # optimise for gradient 0
    outputs = model_word2vec_CBOW(X_train_tensor_word2vec_CBOW) # predict the output based on our model
    loss = criterion(outputs, y_train_tensor) # calculate the loss
    loss.backward() # Compute the gradient
    optimizer_CBOW.step() # update the weights
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Train the model for TF-IDF
for epoch in range(10):  # Number of epochs
    optimizer_tfidf.zero_grad() # optimise for gradient 0
    outputs = model_tfidf(X_train_tensor_tfidf) # predict the output based on our model
    loss = criterion(outputs, y_train_tensor) # calculate the loss
    loss.backward() # Compute the gradient
    optimizer_tfidf.step() # update the weights
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Evaluation of Perceptron

"""
First we convert the data into word embedding and then convert it into tensors.
We then make predictions and then convert them into labels, i.e. stances.
We will then use Micro-average F1 score.
"""

X_test_Glove = np.array([get_sentence_embedding_Glove(tweet, glove_embeddings) for tweet in Tweet_test])
X_test_word2vec_SkipGram = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_SkipGram) for tweet in Tweet_test])
X_test_word2vec_CBOW = np.array([get_sentence_embedding_word2vec(tweet, word2vec_model_CBOW) for tweet in Tweet_test])
X_test_tfidf = vectorizer.transform(Tweet_test)
X_test_tfidf_dense = X_test_tfidf.toarray() #TfidfVectorizer returns a sparse matrix and pytorch tensor requires an array


# Converting data to PyTorch tensors
X_test_tensor_Glove = torch.tensor(X_test_Glove, dtype=torch.float32)
X_test_tensor_word2vec_SkipGram = torch.tensor(X_test_word2vec_SkipGram, dtype=torch.float32)
X_test_tensor_word2vec_CBOW = torch.tensor(X_test_word2vec_CBOW, dtype=torch.float32)
X_test_tensor_tfidf = torch.tensor(X_test_tfidf_dense, dtype=torch.float32)


y_test_tensor = torch.tensor(Stance_test.factorize()[0], dtype=torch.long)

# Make predictions
#Glove
with torch.no_grad():
    outputs = model_Glove(X_test_tensor_Glove)
    _, predicted_Glove = torch.max(outputs, 1)

#SkipGram
with torch.no_grad():
    outputs = model_word2vec_SkipGram(X_test_tensor_word2vec_SkipGram)
    _, predicted_SkipGram = torch.max(outputs, 1)


#CBOW
with torch.no_grad():
    outputs = model_word2vec_CBOW(X_test_tensor_word2vec_CBOW)
    _, predicted_CBOW = torch.max(outputs, 1)

#TF-IDF
with torch.no_grad():
    outputs = model_tfidf(X_test_tensor_tfidf)
    _, predicted_tfidf = torch.max(outputs, 1)


# Convert predictions to labels
predicted_labels_Glove = [["FAVOR", "AGAINST", "NONE"][i] for i in predicted_Glove]
predicted_labels_SkipGram = [["FAVOR", "AGAINST", "NONE"][i] for i in predicted_SkipGram]
predicted_labels_CBOW = [["FAVOR", "AGAINST", "NONE"][i] for i in predicted_CBOW]
predicted_labels_tfidf = [["FAVOR", "AGAINST", "NONE"][i] for i in predicted_tfidf]


print(Counter(predicted_labels_Glove))
print(Counter(predicted_labels_SkipGram))
print(Counter(predicted_labels_CBOW))
print(Counter(predicted_labels_tfidf))

# Calculate F1 score For each
from sklearn.metrics import classification_report, f1_score

print(classification_report(Stance_test, predicted_labels_Glove))
#print("Macro-average F1 score:", f1_score(Stance_test, predicted_labels_Glove, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, predicted_labels_Glove, average='micro'))
print(classification_report(Stance_test, predicted_labels_SkipGram))
#print("Macro-average F1 score:", f1_score(Stance_test, predicted_labels_SkipGram, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, predicted_labels_SkipGram, average='micro'))
print(classification_report(Stance_test, predicted_labels_CBOW))
#print("Macro-average F1 score:", f1_score(Stance_test, predicted_labels_CBOW, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, predicted_labels_CBOW, average='micro'))
print(classification_report(Stance_test, predicted_labels_tfidf))
#print("Macro-average F1 score:", f1_score(Stance_test, predicted_labels_tfidf, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, predicted_labels_tfidf, average='micro'))

# 2nd ML method: Naive Bayes classification

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score
from sklearn.naive_bayes import GaussianNB


# Train Naive Bayes classifier for tfidf
nb_classifier_tfidf = MultinomialNB()
nb_classifier_tfidf.fit(X_train_tfidf, Stance)

# Glove
nb_classifier_Glove = GaussianNB()
nb_classifier_Glove.fit(X_train_Glove, Stance)

# Skipgram
nb_classifier_skipgram = GaussianNB()
nb_classifier_skipgram.fit(X_train_word2vec_SkipGram, Stance)

# CBOW
nb_classifier_cbow = GaussianNB()
nb_classifier_cbow.fit(X_train_word2vec_CBOW, Stance)

# Make predictions
y_pred_tfidf = nb_classifier_tfidf.predict(X_test_tfidf)
y_pred_Glove = nb_classifier_Glove.predict(X_test_Glove)
y_pred_skipgram = nb_classifier_skipgram.predict(X_test_word2vec_SkipGram)
y_pred_cbow = nb_classifier_cbow.predict(X_test_word2vec_CBOW)


# Evaluate performance
print(classification_report(Stance_test, y_pred_Glove))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_Glove, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_Glove, average='micro'))
print(classification_report(Stance_test, y_pred_skipgram))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_skipgram, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_skipgram, average='micro'))
print(classification_report(Stance_test, y_pred_cbow))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_cbow, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_cbow, average='micro'))
print(classification_report(Stance_test, y_pred_tfidf))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_tfidf, average='micro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_tfidf, average='micro'))

#svm

from sklearn.svm import SVC


# Train SVM classifier

#TF-IDF
svm_classifier = SVC(kernel='linear')
svm_classifier.fit(X_train_tfidf, Stance)

#Glove
svm_classifier_Glove = SVC(kernel='linear')
svm_classifier_Glove.fit(X_train_Glove, Stance)

#Skipgram
svm_classifier_skipgram = SVC(kernel='linear')
svm_classifier_skipgram.fit(X_train_word2vec_SkipGram, Stance)

#CBOW
svm_classifier_cbow = SVC(kernel='linear')
svm_classifier_cbow.fit(X_train_word2vec_CBOW, Stance)


# Make predictions
y_pred_tfidf_svm = svm_classifier.predict(X_test_tfidf)
y_pred_Glove_svm = svm_classifier_Glove.predict(X_test_Glove)
y_pred_skipgram_svm = svm_classifier_skipgram.predict(X_test_word2vec_SkipGram)
y_pred_cbow_svm = svm_classifier_cbow.predict(X_test_word2vec_CBOW)

# Evaluate performance
print(classification_report(Stance_test, y_pred_Glove_svm))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_Glove_svm, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_Glove_svm, average='micro'))
print(classification_report(Stance_test, y_pred_skipgram_svm))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_skipgram_svm, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_skipgram_svm, average='micro'))
print(classification_report(Stance_test, y_pred_cbow_svm))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_cbow_svm, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_cbow_svm, average='micro'))
print(classification_report(Stance_test, y_pred_tfidf_svm))
#print("Macro-average F1 score:", f1_score(Stance_test, y_pred_tfidf_svm, average='macro'))
print("Micro-average F1 score:", f1_score(Stance_test, y_pred_tfidf_svm, average='micro'))